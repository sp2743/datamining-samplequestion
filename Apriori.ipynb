{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sp2743/datamining-samplequestion/blob/main/Apriori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ffa93d",
      "metadata": {
        "id": "18ffa93d"
      },
      "source": [
        "### Task 1: Generate 4 random transaction datasets for experiment          [10]\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "    # transactions in each dataset: 20\n",
        "    # items per transaction: range 1-7  [Repreation not allowed]\n",
        "    # items set:- {i1, i2, ------------------- i40}\n",
        "    \n",
        "    Example:-\n",
        "    T1: {i3, i1, i5}\n",
        "    T2: {i6, i12, i3, i40, i11, i9, i33}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "b6kbmnixhDcH"
      },
      "id": "b6kbmnixhDcH",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_transactions():\n",
        "    \"\"\"Generates random transactions for a dataset.\"\"\"\n",
        "    items = [f\"i{x}\" for x in range(1, 41)]  # Items {i1, i2, ..., i40}\n",
        "    transactions = []\n",
        "    for i in range(1, 21):  # 20 transactions per dataset\n",
        "        transaction = random.sample(items, random.randint(1, 7))\n",
        "        transactions.append(f\"T{i}: {{{', '.join(transaction)}}}\")\n",
        "    return transactions\n",
        "\n",
        "# Generate and save four datasets\n",
        "for dataset_num in range(1, 5):\n",
        "    transactions = generate_transactions()\n",
        "    with open(f\"dataset_{dataset_num}.txt\", \"w\") as file:\n",
        "        file.write(\"\\n\".join(transactions))\n",
        "\n",
        "def read_and_print_datasets(num_datasets=4):\n",
        "    \"\"\"Reads datasets from files and prints them in the specified format.\"\"\"\n",
        "    for dataset_num in range(1, num_datasets + 1):\n",
        "        print(f\"dataset{dataset_num}:\")\n",
        "\n",
        "        file_path = f\"dataset_{dataset_num}.txt\"\n",
        "        with open(file_path, \"r\") as file:\n",
        "            for line in file:\n",
        "                print(line.strip().replace(\": \", \"-\"))\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Run the function\n",
        "read_and_print_datasets()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-eA8gWN5UaC",
        "outputId": "bbdaf4b6-58cc-4d41-beb4-1938907bfc4d"
      },
      "id": "C-eA8gWN5UaC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset1:\n",
            "T1-{i27, i6}\n",
            "T2-{i7, i26, i27, i11, i25}\n",
            "T3-{i18, i6, i12}\n",
            "T4-{i31, i11, i27}\n",
            "T5-{i39}\n",
            "T6-{i18, i12, i6, i38}\n",
            "T7-{i3, i18, i37, i9, i24}\n",
            "T8-{i17, i19, i14, i27, i18, i13}\n",
            "T9-{i12}\n",
            "T10-{i5}\n",
            "T11-{i27, i3, i29}\n",
            "T12-{i3, i12, i33, i11, i18}\n",
            "T13-{i5, i18, i16, i17, i36}\n",
            "T14-{i28, i17, i39, i32, i12}\n",
            "T15-{i32, i23, i33, i12, i11, i22}\n",
            "T16-{i20}\n",
            "T17-{i37, i24, i21, i32, i26, i18}\n",
            "T18-{i39}\n",
            "T19-{i12, i7, i8, i17, i20}\n",
            "T20-{i12, i17, i31, i33}\n",
            "----------------------------------------\n",
            "dataset2:\n",
            "T1-{i24, i7, i26, i23, i25}\n",
            "T2-{i16, i37, i28, i31, i6, i32}\n",
            "T3-{i3, i21, i13, i6, i33, i35}\n",
            "T4-{i19, i28}\n",
            "T5-{i24, i33}\n",
            "T6-{i34}\n",
            "T7-{i2, i11, i30, i21, i35}\n",
            "T8-{i5, i17, i27, i38, i33, i10, i3}\n",
            "T9-{i26}\n",
            "T10-{i17, i8, i7, i32, i28, i20}\n",
            "T11-{i6, i2, i3, i36, i34}\n",
            "T12-{i10, i5, i33, i24, i9, i27, i11}\n",
            "T13-{i25, i36}\n",
            "T14-{i35, i36, i12, i4, i39, i30, i10}\n",
            "T15-{i37, i40, i28, i22, i3}\n",
            "T16-{i17, i16, i36, i34, i3, i11}\n",
            "T17-{i28, i13, i21}\n",
            "T18-{i21, i25, i30, i12, i37, i9}\n",
            "T19-{i20, i25, i21, i10, i4, i19}\n",
            "T20-{i11, i27, i5, i6, i35}\n",
            "----------------------------------------\n",
            "dataset3:\n",
            "T1-{i1, i36, i38, i8, i19}\n",
            "T2-{i39, i15, i31, i4}\n",
            "T3-{i25}\n",
            "T4-{i13, i31}\n",
            "T5-{i28}\n",
            "T6-{i7}\n",
            "T7-{i8, i9, i34, i39, i4}\n",
            "T8-{i6, i40, i38, i16, i28}\n",
            "T9-{i19}\n",
            "T10-{i31, i23}\n",
            "T11-{i4}\n",
            "T12-{i10, i39, i1, i23}\n",
            "T13-{i9, i15, i23, i16}\n",
            "T14-{i23}\n",
            "T15-{i3}\n",
            "T16-{i12, i6, i21, i4, i17, i30}\n",
            "T17-{i11, i10, i15, i14, i13, i12}\n",
            "T18-{i24, i26}\n",
            "T19-{i30, i23, i1, i20, i25}\n",
            "T20-{i34, i40, i9, i31, i12}\n",
            "----------------------------------------\n",
            "dataset4:\n",
            "T1-{i39}\n",
            "T2-{i34, i31}\n",
            "T3-{i6, i1, i36, i10, i5}\n",
            "T4-{i3, i17}\n",
            "T5-{i18, i24, i39, i3, i23}\n",
            "T6-{i40, i31, i35, i12, i6, i37}\n",
            "T7-{i34, i15, i9, i13, i3, i25}\n",
            "T8-{i3, i32, i17, i8, i35}\n",
            "T9-{i15, i2}\n",
            "T10-{i10, i4, i20, i30, i7, i15, i17}\n",
            "T11-{i28}\n",
            "T12-{i24, i40, i7, i19, i30}\n",
            "T13-{i12}\n",
            "T14-{i40, i23, i21, i36, i10, i27}\n",
            "T15-{i33, i14}\n",
            "T16-{i29, i21, i16, i2, i39, i12, i5}\n",
            "T17-{i35, i18, i13}\n",
            "T18-{i29, i33, i11, i16, i28, i3, i8}\n",
            "T19-{i38, i29, i7, i10, i13, i14, i32}\n",
            "T20-{i35, i37}\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4939ba55",
      "metadata": {
        "id": "4939ba55"
      },
      "source": [
        "### Task 2: Implementing Apriori Algorithm from Scratch         [20]\n",
        "\n",
        "Write a Python program to implement the Apriori algorithm to find frequent itemsets and generate association rules from a given transaction dataset.\n",
        "\n",
        "\n",
        "    Accept transactions as input (manually or from a file).\n",
        "    Set a minimum support threshold and confidence threshold.\n",
        "    Implement candidate generation, pruning, and rule generation.\n",
        "    Display frequent itemsets and association rules."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_transactions(file_path):\n",
        "    \"\"\"Loads transactions from a file and returns a list of sets.\"\"\"\n",
        "    transactions = []\n",
        "    with open(file_path, \"r\") as file:\n",
        "        for line in file:\n",
        "            items = line.strip().split(\": \")[1]  # Extract items after \"T1: \"\n",
        "            items = items.strip(\"{}\").split(\", \")  # Remove {} and split items\n",
        "            transactions.append(set(items))\n",
        "    return transactions"
      ],
      "metadata": {
        "id": "FePy7hI_5Tnn"
      },
      "id": "FePy7hI_5Tnn",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def get_frequent_itemsets(transactions, min_support):\n",
        "    \"\"\"Finds frequent itemsets with given min_support.\"\"\"\n",
        "    item_count = {}\n",
        "    total_transactions = len(transactions)\n",
        "\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            item_count[item] = item_count.get(item, 0) + 1\n",
        "\n",
        "    return {frozenset([item]): count for item, count in item_count.items() if count / total_transactions >= min_support}\n",
        "\n",
        "def new_candidates(frequent_itemsets):\n",
        "    \"\"\"Generates new candidate itemsets from frequent itemsets.\"\"\"\n",
        "    candidates = {}\n",
        "    items = list({item for itemset in frequent_itemsets for item in itemset})\n",
        "\n",
        "    for i in range(len(items)):\n",
        "        for j in range(i+1, len(items)):\n",
        "            new_item = frozenset([items[i], items[j]])\n",
        "            if new_item not in candidates:\n",
        "                candidates[new_item] = 0\n",
        "    return candidates\n",
        "\n",
        "\n",
        "def prune_candidates(candidates, frequent_itemsets):\n",
        "    \"\"\"Prunes candidates that have infrequent subsets.\"\"\"\n",
        "    pruned_candidates = {}\n",
        "    for candidate in candidates:\n",
        "        if all(frozenset([item]) in frequent_itemsets for item in candidate):\n",
        "            pruned_candidates[candidate] = 0\n",
        "    return pruned_candidates\n",
        "\n",
        "def generate_association_rules(frequent_itemsets, min_confidence):\n",
        "    \"\"\"Generates association rules from frequent itemsets.\"\"\"\n",
        "    rules = []\n",
        "    for itemset, support in frequent_itemsets.items():\n",
        "        if len(itemset) > 1:\n",
        "            for i in range(1, len(itemset)):\n",
        "                for antecedent in itertools.combinations(itemset, i):\n",
        "                    antecedent = frozenset(antecedent)\n",
        "                    consequent = itemset - antecedent\n",
        "                    if consequent and antecedent in frequent_itemsets:\n",
        "                        confidence = support / frequent_itemsets[antecedent]\n",
        "                        if confidence >= min_confidence:\n",
        "                            rules.append((antecedent, consequent, confidence))\n",
        "    return rules"
      ],
      "metadata": {
        "id": "yPm98MbDBfJV"
      },
      "id": "yPm98MbDBfJV",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"dataset_1.txt\"\n",
        "transactions = load_transactions(file_path)\n",
        "\n",
        "min_support = 0.2\n",
        "min_confidence = 0.5\n",
        "\n",
        "# Find frequent itemsets\n",
        "frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
        "\n",
        "print(\"\\nFrequent Itemsets:\")\n",
        "for itemset, support in frequent_itemsets.items():\n",
        "    print(f\"{set(itemset)}: Support = {support}\")\n",
        "\n",
        "\n",
        "candidates = new_candidates(frequent_itemsets)\n",
        "pruned_candidates = prune_candidates(candidates, frequent_itemsets)\n",
        "\n",
        "\n",
        "rules = generate_association_rules(pruned_candidates, min_confidence)\n",
        "\n",
        "print(\"\\nAssociation Rules:\")\n",
        "for antecedent, consequent, confidence in rules:\n",
        "    print(f\"Rule: {set(antecedent)} -> {set(consequent)}, Confidence = {confidence:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfA6RKhdET1e",
        "outputId": "5cf1b0f8-800d-47df-adf4-debc8b166cd1"
      },
      "id": "TfA6RKhdET1e",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequent Itemsets:\n",
            "{'i27'}: Support = 5\n",
            "{'i11'}: Support = 4\n",
            "{'i12'}: Support = 8\n",
            "{'i18'}: Support = 7\n",
            "{'i17'}: Support = 5\n",
            "\n",
            "Association Rules:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157f4103",
      "metadata": {
        "id": "157f4103"
      },
      "source": [
        "### Task 3: Applying Apriori using Python Libraries (MLxtend)   [20]\n",
        "Use the MLxtend library to find frequent itemsets and generate association rules.\n",
        "\n",
        "    Use mlxtend.frequent_patterns.apriori to extract frequent itemsets.\n",
        "    Use mlxtend.frequent_patterns.association_rules to generate association rules.\n",
        "    \n",
        "    For more details:- https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "def encode_transactions(transactions):\n",
        "    \"\"\"Encodes transactions into a DataFrame for MLxtend's Apriori.\"\"\"\n",
        "    unique_items = sorted({item for transaction in transactions for item in transaction})\n",
        "    encoded_data = [{item: (item in transaction) for item in unique_items} for transaction in transactions]\n",
        "    return pd.DataFrame(encoded_data)\n",
        "\n",
        "def run_apriori_mlxtend(df, min_support, min_confidence):\n",
        "    \"\"\"Runs MLxtend's Apriori algorithm.\"\"\"\n",
        "    frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
        "    if frequent_itemsets.empty:\n",
        "        return 0, 0\n",
        "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "    return len(frequent_itemsets), len(rules)"
      ],
      "metadata": {
        "id": "qBIjUnzRIrrp"
      },
      "id": "qBIjUnzRIrrp",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"dataset_1.txt\"\n",
        "transactions = load_transactions(file_path)\n",
        "\n",
        "\n",
        "df = encode_transactions(transactions)\n",
        "\n",
        "\n",
        "min_support = 0.2\n",
        "min_confidence = 0.5\n",
        "\n",
        "num_frequent_itemsets, num_rules = run_apriori_mlxtend(df, min_support, min_confidence)\n",
        "\n",
        "\n",
        "print(f\"\\nTotal Frequent Itemsets: {num_frequent_itemsets}\")\n",
        "print(f\"Total Association Rules: {num_rules}\")\n",
        "\n",
        "# Obtain frequent itemsets and rules using apriori and association_rules\n",
        "frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
        "if not frequent_itemsets.empty:\n",
        "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "\n",
        "print(\"\\nFrequent Itemsets:\")\n",
        "print(frequent_itemsets)\n",
        "\n",
        "if rules is not None and not rules.empty:\n",
        "    print(\"\\nAssociation Rules:\")\n",
        "    print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSSL-lBADdTb",
        "outputId": "9f6a9363-7a4f-4f26-c9cd-56cdebca3b04"
      },
      "id": "VSSL-lBADdTb",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Frequent Itemsets: 5\n",
            "Total Association Rules: 0\n",
            "\n",
            "Frequent Itemsets:\n",
            "   support itemsets\n",
            "0     0.20    (i11)\n",
            "1     0.40    (i12)\n",
            "2     0.25    (i17)\n",
            "3     0.35    (i18)\n",
            "4     0.25    (i27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39dddf72",
      "metadata": {
        "id": "39dddf72"
      },
      "source": [
        "### Task 4: Experimenting with Different Support & Confidence Levels [Self-Implemented & In-Built]\n",
        "\n",
        "Analyze how changing minimum support and confidence thresholds affects the number of frequent itemsets and association rules.\n",
        "\n",
        "For each dataset:-\n",
        "\n",
        "    Run the Apriori algorithm with different min_support and min_confidence values:-          [20]\n",
        "    (0.2, 0.4),  (0.5, 0.7), (0.8, 0.9)\n",
        "    \n",
        "    Compare the number of frequent itemsets and association rules generated (tabular format). [10]\n",
        "    \n",
        "    \n",
        "#### Write final observations on how the results change. [20]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "support_confidence_pairs = [(0.2, 0.4), (0.5, 0.7), (0.8, 0.9)]\n",
        "results = []\n",
        "\n",
        "for dataset_num in range(1, 5):\n",
        "    file_path = f\"dataset_{dataset_num}.txt\"\n",
        "    transactions = load_transactions(file_path)\n",
        "    df = encode_transactions(transactions)\n",
        "\n",
        "    for min_support, min_confidence in support_confidence_pairs:\n",
        "        # Run Manual Apriori\n",
        "        frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
        "        candidates = new_candidates(frequent_itemsets)\n",
        "        pruned_candidates = prune_candidates(candidates, frequent_itemsets)\n",
        "        rules = generate_association_rules(pruned_candidates, min_confidence)\n",
        "\n",
        "        # Run MLxtend Apriori\n",
        "        mlxtend_frequent, mlxtend_rules = run_apriori_mlxtend(df, min_support, min_confidence)\n",
        "\n",
        "        # Store results\n",
        "        results.append([dataset_num, min_support, min_confidence, len(pruned_candidates), len(rules), mlxtend_frequent, mlxtend_rules])\n",
        "\n",
        "# Convert results to DataFrame and save\n",
        "results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Min Support\", \"Min Confidence\",\n",
        "                                            \"Manual Frequent Itemsets\", \"Manual Rules\",\n",
        "                                            \"MLxtend Frequent Itemsets\", \"MLxtend Rules\"])\n",
        "results_df.to_csv(\"apriori_results.csv\", index=False)\n",
        "\n",
        "# Print Summary\n",
        "print(\"\\nComparison of Manual vs MLxtend Apriori Results:\")\n",
        "print(results_df.to_markdown())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWSAVn7EK_59",
        "outputId": "821e0484-2784-4e76-d51d-d6630a086d35"
      },
      "id": "oWSAVn7EK_59",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of Manual vs MLxtend Apriori Results:\n",
            "|    |   Dataset |   Min Support |   Min Confidence |   Manual Frequent Itemsets |   Manual Rules |   MLxtend Frequent Itemsets |   MLxtend Rules |\n",
            "|---:|----------:|--------------:|-----------------:|---------------------------:|---------------:|----------------------------:|----------------:|\n",
            "|  0 |         1 |           0.2 |              0.4 |                         10 |              0 |                           5 |               0 |\n",
            "|  1 |         1 |           0.5 |              0.7 |                          0 |              0 |                           0 |               0 |\n",
            "|  2 |         1 |           0.8 |              0.9 |                          0 |              0 |                           0 |               0 |\n",
            "|  3 |         2 |           0.2 |              0.4 |                         45 |              0 |                          10 |               0 |\n",
            "|  4 |         2 |           0.5 |              0.7 |                          0 |              0 |                           0 |               0 |\n",
            "|  5 |         2 |           0.8 |              0.9 |                          0 |              0 |                           0 |               0 |\n",
            "|  6 |         3 |           0.2 |              0.4 |                          3 |              0 |                           3 |               0 |\n",
            "|  7 |         3 |           0.5 |              0.7 |                          0 |              0 |                           0 |               0 |\n",
            "|  8 |         3 |           0.8 |              0.9 |                          0 |              0 |                           0 |               0 |\n",
            "|  9 |         4 |           0.2 |              0.4 |                          3 |              0 |                           3 |               0 |\n",
            "| 10 |         4 |           0.5 |              0.7 |                          0 |              0 |                           0 |               0 |\n",
            "| 11 |         4 |           0.8 |              0.9 |                          0 |              0 |                           0 |               0 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis\n",
        "### Higher Support Thresholds Reduce Frequent Itemsets\n",
        "### Higher Confidence Thresholds Reduce\n",
        "### Low confidence (0.4) allows more association rules, while high confidence (0.7 or 0.9) often prevents any rule generation"
      ],
      "metadata": {
        "id": "oTbA_mAMLTPx"
      },
      "id": "oTbA_mAMLTPx"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}